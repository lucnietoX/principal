{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Pyspark - First Touch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows the first touch with Pyspark and some basic & fundamental commands.\n",
    "\n",
    "What is Pyspark? \n",
    "PySpark is the Python API for Spark.\n",
    "\n",
    "Prerequisites for run the following code:\n",
    "\n",
    "Installed & Configured: Spark + Pyspark + Python\n",
    "\n",
    "For the configuration, I suggest this source: https://docs.anaconda.com/anaconda-scale/howto/spark-configuration/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Luciano Nieto    |     Date: 05/06/20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# The first step is create a Spark Session, where its possible to configure the cluster nodes, \n",
    "# and the memory allocated to each one.\n",
    "\n",
    "# Init your spark session:\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "   .master(\"local\") \\\n",
    "   .appName(\"My First App\") \\\n",
    "   .config(\"spark.executor.memory\", \"1gb\") \\\n",
    "   .getOrCreate()\n",
    "   \n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 11]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RDD: Resilient Distributed Datasets, Spark revolves around the concept of a resilient distributed dataset (RDD), \n",
    "# which is a fault-tolerant collection of elements that can be operated on in parallel. \n",
    "# There are two ways to create RDDs: parallelizing an existing collection in your driver program, or referencing \n",
    "# a dataset in an external storage system, such as a shared filesystem, HDFS, HBase, or any data source \n",
    "# offering a Hadoop InputFormat. Source: https://spark.apache.org/docs/latest/rdd-programming-guide.html\n",
    "\n",
    "# 2 ways to create the RDD:\n",
    "# Parallelize: From Collection\n",
    "# TextFile: From External Data\n",
    "\n",
    "\n",
    "# From Collection! \n",
    "rdd = sc.parallelize([10,11,12,13,14],2)\n",
    "\n",
    "# Visualize the content of RDD:\n",
    "rdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['customerID,gender,SeniorCitizen,Partner,Dependents,tenure,PhoneService,MultipleLines,InternetService,OnlineSecurity,OnlineBackup,DeviceProtection,TechSupport,StreamingTV,StreamingMovies,Contract,PaperlessBilling,PaymentMethod,MonthlyCharges,TotalCharges,Churn',\n",
       " '7590-VHVEG,Female,0,Yes,No,1,No,No phone service,DSL,No,Yes,No,No,No,No,Month-to-month,Yes,Electronic check,29.85,29.85,No',\n",
       " '5575-GNVDE,Male,0,No,No,34,Yes,No,DSL,Yes,No,Yes,No,No,No,One year,No,Mailed check,56.95,1889.5,No']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#RDD from External Data !\n",
    "\n",
    "#a) Lets first take the dataset: source: https://www.kaggle.com/blastchar/telco-customer-churn/data\n",
    "#b) Read the Data from CSV to RDD:\n",
    "\n",
    "file = 'data/WA_Fn-UseC_-Telco-Customer-Churn.csv' #<file location + filename>\n",
    "\n",
    "rdd = sc.textFile(file)\n",
    "\n",
    "#c) See the content of the RDD - first 3 elements:\n",
    "rdd.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________________________________________________________________________________________________\n",
    "\n",
    "\n",
    "<h4> RDD Transformation: produces a new rdd with the applied transformation. </h4>\n",
    "\n",
    "  Narrow Transformation: Calculate all the elements at the same partition.\n",
    "  Commands: Map,FlatMap,MapPartition,Filter,Sample,Union...\n",
    "\n",
    "  Wide Transformation: Calculate all the elements in a single partition or may live in anothers.\n",
    "  Commands: Intersection,Distinct,ReduceByKey,GroupByKey,Join,Cartesian,Repartition,Coalesce..\n",
    "  \n",
    "  \n",
    "  \n",
    "<h4> RDD Action: work with the actual dataset operations. </h4>\n",
    "  Commands: Reduce, Collect, Count, First, Take, CountByKey...\n",
    "  \n",
    "More: https://spark.apache.org/docs/latest/rdd-programming-guide.html\n",
    "\n",
    "\n",
    "_________________________________________________________________________________________________________\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'customerID,gender,SeniorCitizen,Partner,Dependents,tenure,PhoneService,MultipleLines,InternetService,OnlineSecurity,OnlineBackup,DeviceProtection,TechSupport,StreamingTV,StreamingMovies,Contract,PaperlessBilling,PaymentMethod,MonthlyCharges,TotalCharges,Churn'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RDD action:\n",
    "# First element\n",
    "\n",
    "rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[259, 122, 98]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Length of the first 3 elements:\n",
    "\n",
    "rdd.map(lambda s: len(s)).take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['customerID',\n",
       "  'gender',\n",
       "  'SeniorCitizen',\n",
       "  'Partner',\n",
       "  'Dependents',\n",
       "  'tenure',\n",
       "  'PhoneService',\n",
       "  'MultipleLines',\n",
       "  'InternetService',\n",
       "  'OnlineSecurity',\n",
       "  'OnlineBackup',\n",
       "  'DeviceProtection',\n",
       "  'TechSupport',\n",
       "  'StreamingTV',\n",
       "  'StreamingMovies',\n",
       "  'Contract',\n",
       "  'PaperlessBilling',\n",
       "  'PaymentMethod',\n",
       "  'MonthlyCharges',\n",
       "  'TotalCharges',\n",
       "  'Churn'],\n",
       " ['7590-VHVEG',\n",
       "  'Female',\n",
       "  '0',\n",
       "  'Yes',\n",
       "  'No',\n",
       "  '1',\n",
       "  'No',\n",
       "  'No phone service',\n",
       "  'DSL',\n",
       "  'No',\n",
       "  'Yes',\n",
       "  'No',\n",
       "  'No',\n",
       "  'No',\n",
       "  'No',\n",
       "  'Month-to-month',\n",
       "  'Yes',\n",
       "  'Electronic check',\n",
       "  '29.85',\n",
       "  '29.85',\n",
       "  'No']]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split lines by comma. When the RDD is created, all the lines are inside a string.\n",
    "# With the command Map, we can separate each field by a comma, for instance.\n",
    "\n",
    "rdd_splited = rdd.map(lambda line: line.split(\",\"))\n",
    "rdd_splited.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['3668-QPYBK',\n",
       "  'Male',\n",
       "  '0',\n",
       "  'No',\n",
       "  'No',\n",
       "  '2',\n",
       "  'Yes',\n",
       "  'No',\n",
       "  'DSL',\n",
       "  'Yes',\n",
       "  'Yes',\n",
       "  'No',\n",
       "  'No',\n",
       "  'No',\n",
       "  'No',\n",
       "  'Month-to-month',\n",
       "  'Yes',\n",
       "  'Mailed check',\n",
       "  '53.85',\n",
       "  '108.15',\n",
       "  'Yes']]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Filter: filter the rdd based in a condition:\n",
    "\n",
    "rdd_churns = rdd.filter(lambda x: x[20] == \"Yes\")\n",
    "rdd_churns.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['9237-HQITU',\n",
       "  'Female',\n",
       "  '0',\n",
       "  'No',\n",
       "  'No',\n",
       "  '2',\n",
       "  'Yes',\n",
       "  'No',\n",
       "  'Fiber optic',\n",
       "  'No',\n",
       "  'No',\n",
       "  'No',\n",
       "  'No',\n",
       "  'No',\n",
       "  'No',\n",
       "  'Month-to-month',\n",
       "  'Yes',\n",
       "  'Electronic check',\n",
       "  '70.7',\n",
       "  '151.65',\n",
       "  'Yes']]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Filter: \"in\" line:\n",
    "\n",
    "rdd_fiber = rdd.filter(lambda x: \"Fiber optic\" in x)\n",
    "rdd_fiber.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['customerID',\n",
       "  'gender',\n",
       "  'SeniorCitizen',\n",
       "  'Partner',\n",
       "  'Dependents',\n",
       "  'tenure',\n",
       "  'PhoneService',\n",
       "  'MultipleLines',\n",
       "  'InternetService',\n",
       "  'OnlineSecurity',\n",
       "  'OnlineBackup',\n",
       "  'DeviceProtection',\n",
       "  'TechSupport',\n",
       "  'StreamingTV',\n",
       "  'StreamingMovies',\n",
       "  'Contract',\n",
       "  'PaperlessBilling',\n",
       "  'PaymentMethod',\n",
       "  'MonthlyCharges',\n",
       "  'TotalCharges',\n",
       "  'Churn'],\n",
       " ['9093-FPDLG',\n",
       "  'Female',\n",
       "  '0',\n",
       "  'No',\n",
       "  'No',\n",
       "  '11',\n",
       "  'Yes',\n",
       "  'No',\n",
       "  'Fiber optic',\n",
       "  'No',\n",
       "  'Yes',\n",
       "  'Yes',\n",
       "  'Yes',\n",
       "  'No',\n",
       "  'Yes',\n",
       "  'Month-to-month',\n",
       "  'Yes',\n",
       "  'Electronic check',\n",
       "  '94.2',\n",
       "  '999.9',\n",
       "  'No']]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sort the rdd by the Total Changes columns - desccending:\n",
    "\n",
    "rdd_sorted = rdd.sortBy(lambda line: line[19],ascending = False)\n",
    "rdd_sorted.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['customerID',\n",
       "   'gender',\n",
       "   'SeniorCitizen',\n",
       "   'Partner',\n",
       "   'Dependents',\n",
       "   'tenure',\n",
       "   'PhoneService',\n",
       "   'MultipleLines',\n",
       "   'InternetService',\n",
       "   'OnlineSecurity',\n",
       "   'OnlineBackup',\n",
       "   'DeviceProtection',\n",
       "   'TechSupport',\n",
       "   'StreamingTV',\n",
       "   'StreamingMovies',\n",
       "   'Contract',\n",
       "   'PaperlessBilling'],\n",
       "  ['MonthlyCharges', 'TotalCharges']),\n",
       " (['7590-VHVEG',\n",
       "   'Female',\n",
       "   '0',\n",
       "   'Yes',\n",
       "   'No',\n",
       "   '1',\n",
       "   'No',\n",
       "   'No phone service',\n",
       "   'DSL',\n",
       "   'No',\n",
       "   'Yes',\n",
       "   'No',\n",
       "   'No',\n",
       "   'No',\n",
       "   'No',\n",
       "   'Month-to-month',\n",
       "   'Yes'],\n",
       "  ['29.85', '29.85'])]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Key Value pairs: RDD: to perform aggregations, and transform the data.\n",
    "\n",
    "\n",
    "rdd_2 = rdd_splited.map(lambda line: (line[0:17],line[18:20]))\n",
    "rdd_2.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> RDD x DataFrame x DataSets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDD: Primary user-facing API in Spark, since the beggining. About 2011.\n",
    "\n",
    "DataFrame: Distribute collection of Row objects, UDFs, logical plan optimizer. Spark 2.0. 2015.\n",
    "\n",
    "Dataset: Starting in Spark 2.0. Strongly-typed API & performed. 2016 (Scala & Java, only unitl now***).\n",
    "\n",
    "Source: https://spark.apache.org/docs/latest/sql-programming-guide.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> PySpark DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_1='customerID', _2='gender', _3='SeniorCitizen', _4='Partner', _5='Dependents', _6='tenure', _7='PhoneService', _8='MultipleLines', _9='InternetService', _10='OnlineSecurity', _11='OnlineBackup', _12='DeviceProtection', _13='TechSupport', _14='StreamingTV', _15='StreamingMovies', _16='Contract', _17='PaperlessBilling', _18='PaymentMethod', _19='MonthlyCharges', _20='TotalCharges', _21='Churn'),\n",
       " Row(_1='7590-VHVEG', _2='Female', _3='0', _4='Yes', _5='No', _6='1', _7='No', _8='No phone service', _9='DSL', _10='No', _11='Yes', _12='No', _13='No', _14='No', _15='No', _16='Month-to-month', _17='Yes', _18='Electronic check', _19='29.85', _20='29.85', _21='No')]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There are many ways to create DataFrames in Pyspark. Lets see how it works:\n",
    "\n",
    "#a) create dataframe based on rdd. Just for an example:\n",
    "\n",
    "df = rdd_splited.toDF()\n",
    "df.head(2) #show values from dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_1=1, _2=2, _3=3), Row(_1=4, _2=5, _3=6)]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#b) create dataframe based on a collection:\n",
    "\n",
    "c = [(1,2,3),(4,5,6),(7,8,9),(10,11,12),(13,14,15)]\n",
    "df = spark.createDataFrame(c)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_c0='customerID', _c1='gender', _c2='SeniorCitizen', _c3='Partner', _c4='Dependents', _c5='tenure', _c6='PhoneService', _c7='MultipleLines', _c8='InternetService', _c9='OnlineSecurity', _c10='OnlineBackup', _c11='DeviceProtection', _c12='TechSupport', _c13='StreamingTV', _c14='StreamingMovies', _c15='Contract', _c16='PaperlessBilling', _c17='PaymentMethod', _c18='MonthlyCharges', _c19='TotalCharges', _c20='Churn'),\n",
       " Row(_c0='7590-VHVEG', _c1='Female', _c2='0', _c3='Yes', _c4='No', _c5='1', _c6='No', _c7='No phone service', _c8='DSL', _c9='No', _c10='Yes', _c11='No', _c12='No', _c13='No', _c14='No', _c15='Month-to-month', _c16='Yes', _c17='Electronic check', _c18='29.85', _c19='29.85', _c20='No')]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#c) create dataframe based on csv file (external):\n",
    "\n",
    "df = spark.read.csv(file)\n",
    "df.head(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
