{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Pyspark - First Touch"   
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows the first touch with Pyspark and some basic & fundamental commands.\n",
    "\n",
    "What is Pyspark? \n",
    "PySpark is the Python API for Spark.\n",
    "\n",
    "Prerequisites for run the following code:\n",
    "\n",
    "Installed & Configured: Spark + Pyspark + Python\n",
    "\n",
    "For the configuration, I suggest this source: https://docs.anaconda.com/anaconda-scale/howto/spark-configuration/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Luciano Nieto    |     Date: 05/06/20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# The first step is create a Spark Session, where its possible to configure the cluster nodes, \n",
    "# and the memory allocated to each one.\n",
    "\n",
    "# Init your spark session:\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "   .master(\"local\") \\\n",
    "   .appName(\"My First App\") \\\n",
    "   .config(\"spark.executor.memory\", \"1gb\") \\\n",
    "   .getOrCreate()\n",
    "   \n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 11]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RDD: Resilient Distributed Datasets, Spark revolves around the concept of a resilient distributed dataset (RDD), \n",
    "# which is a fault-tolerant collection of elements that can be operated on in parallel.. \n",
    "# There are two ways to create RDDs: parallelizing an existing collection in your driver program, or referencing \n",
    "# a dataset in an external storage system, such as a shared filesystem, HDFS, HBase, or any data source \n",
    "# offering a Hadoop InputFormat. Source.: https://spark.apache.org/docs/latest/rdd-programming-guide.html\n",
    "\n",
    "# 2 ways to create the RDD:\n",
    "# Parallelize: From Collection\n",
    "# TextFile: From External Data\n",
    "\n",
    "\n",
    "# From Collection! \n",
    "rdd = sc.parallelize([10,11,12,13,14],2)\n",
    "\n",
    "# Visualize the content of RDD:\n",
    "rdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['customerID,gender,SeniorCitizen,Partner,Dependents,tenure,PhoneService,MultipleLines,InternetService,OnlineSecurity,OnlineBackup,DeviceProtection,TechSupport,StreamingTV,StreamingMovies,Contract,PaperlessBilling,PaymentMethod,MonthlyCharges,TotalCharges,Churn',\n",
       " '7590-VHVEG,Female,0,Yes,No,1,No,No phone service,DSL,No,Yes,No,No,No,No,Month-to-month,Yes,Electronic check,29.85,29.85,No',\n",
       " '5575-GNVDE,Male,0,No,No,34,Yes,No,DSL,Yes,No,Yes,No,No,No,One year,No,Mailed check,56.95,1889.5,No']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#RDD from External Data !\n",
    "\n",
    "#a) Lets first take the dataset: source: https://www.kaggle.com/blastchar/telco-customer-churn/data\n",
    "#b) Read the Data from CSV to RDD:\n",
    "\n",
    "file = 'data/WA_Fn-UseC_-Telco-Customer-Churn.csv' #<file location + filename>\n",
    "\n",
    "rdd = sc.textFile(file)\n",
    "\n",
    "#c) See the content of the RDD - first 3 elements:\n",
    "rdd.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________________________________________________________________________________________________\n",
    "\n",
    "\n",
    "<h4> RDD Transformation: produces a new rdd with the applied transformation. </h4>\n",
    "\n",
    "  Narrow Transformation: Calculate all the elements at the same partition.\n",
    "  Commands: Map,FlatMap,MapPartition,Filter,Sample,Union...\n",
    "\n",
    "  Wide Transformation: Calculate all the elements in a single partition or may live in anothers.\n",
    "  Commands: Intersection,Distinct,ReduceByKey,GroupByKey,Join,Cartesian,Repartition,Coalesce..\n",
    "  \n",
    "  \n",
    "  \n",
    "<h4> RDD Action: work with the actual dataset operations. </h4>\n",
    "  Commands: Reduce, Collect, Count, First, Take, CountByKey...\n",
    "  \n",
    "More: https://spark.apache.org/docs/latest/rdd-programming-guide.html\n",
    "\n",
    "\n",
    "_________________________________________________________________________________________________________\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'customerID,gender,SeniorCitizen,Partner,Dependents,tenure,PhoneService,MultipleLines,InternetService,OnlineSecurity,OnlineBackup,DeviceProtection,TechSupport,StreamingTV,StreamingMovies,Contract,PaperlessBilling,PaymentMethod,MonthlyCharges,TotalCharges,Churn'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RDD action:\n",
    "# First element\n",
    "\n",
    "rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[259, 122, 98]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Length of the first 3 elements:\n",
    "\n",
    "rdd.map(lambda s: len(s)).take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['customerID',\n",
       "  'gender',\n",
       "  'SeniorCitizen',\n",
       "  'Partner',\n",
       "  'Dependents',\n",
       "  'tenure',\n",
       "  'PhoneService',\n",
       "  'MultipleLines',\n",
       "  'InternetService',\n",
       "  'OnlineSecurity',\n",
       "  'OnlineBackup',\n",
       "  'DeviceProtection',\n",
       "  'TechSupport',\n",
       "  'StreamingTV',\n",
       "  'StreamingMovies',\n",
       "  'Contract',\n",
       "  'PaperlessBilling',\n",
       "  'PaymentMethod',\n",
       "  'MonthlyCharges',\n",
       "  'TotalCharges',\n",
       "  'Churn'],\n",
       " ['7590-VHVEG',\n",
       "  'Female',\n",
       "  '0',\n",
       "  'Yes',\n",
       "  'No',\n",
       "  '1',\n",
       "  'No',\n",
       "  'No phone service',\n",
       "  'DSL',\n",
       "  'No',\n",
       "  'Yes',\n",
       "  'No',\n",
       "  'No',\n",
       "  'No',\n",
       "  'No',\n",
       "  'Month-to-month',\n",
       "  'Yes',\n",
       "  'Electronic check',\n",
       "  '29.85',\n",
       "  '29.85',\n",
       "  'No']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split lines by comma. When the RDD is created, all the lines are inside a string.\n",
    "# With the command Map, we can separate each field by a comma, for instance.\n",
    "\n",
    "rdd_splited = rdd.map(lambda line: line.split(\",\"))\n",
    "rdd_splited.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['3668-QPYBK',\n",
       "  'Male',\n",
       "  '0',\n",
       "  'No',\n",
       "  'No',\n",
       "  '2',\n",
       "  'Yes',\n",
       "  'No',\n",
       "  'DSL',\n",
       "  'Yes',\n",
       "  'Yes',\n",
       "  'No',\n",
       "  'No',\n",
       "  'No',\n",
       "  'No',\n",
       "  'Month-to-month',\n",
       "  'Yes',\n",
       "  'Mailed check',\n",
       "  '53.85',\n",
       "  '108.15',\n",
       "  'Yes'],\n",
       " ['9237-HQITU',\n",
       "  'Female',\n",
       "  '0',\n",
       "  'No',\n",
       "  'No',\n",
       "  '2',\n",
       "  'Yes',\n",
       "  'No',\n",
       "  'Fiber optic',\n",
       "  'No',\n",
       "  'No',\n",
       "  'No',\n",
       "  'No',\n",
       "  'No',\n",
       "  'No',\n",
       "  'Month-to-month',\n",
       "  'Yes',\n",
       "  'Electronic check',\n",
       "  '70.7',\n",
       "  '151.65',\n",
       "  'Yes'],\n",
       " ['9305-CDSKC',\n",
       "  'Female',\n",
       "  '0',\n",
       "  'No',\n",
       "  'No',\n",
       "  '8',\n",
       "  'Yes',\n",
       "  'Yes',\n",
       "  'Fiber optic',\n",
       "  'No',\n",
       "  'No',\n",
       "  'Yes',\n",
       "  'No',\n",
       "  'Yes',\n",
       "  'Yes',\n",
       "  'Month-to-month',\n",
       "  'Yes',\n",
       "  'Electronic check',\n",
       "  '99.65',\n",
       "  '820.5',\n",
       "  'Yes']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Filter: filter the rdd based in a condition:\n",
    "\n",
    "rdd_churns = rdd_splited.filter(lambda x: x[20] == \"Yes\")\n",
    "rdd_churns.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['9237-HQITU,Female,0,No,No,2,Yes,No,Fiber optic,No,No,No,No,No,No,Month-to-month,Yes,Electronic check,70.7,151.65,Yes',\n",
       " '9305-CDSKC,Female,0,No,No,8,Yes,Yes,Fiber optic,No,No,Yes,No,Yes,Yes,Month-to-month,Yes,Electronic check,99.65,820.5,Yes',\n",
       " '1452-KIOVK,Male,0,No,Yes,22,Yes,Yes,Fiber optic,No,Yes,No,No,Yes,No,Month-to-month,Yes,Credit card (automatic),89.1,1949.4,No']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Filter: \"in\" line:\n",
    "\n",
    "rdd_fiber = rdd.filter(lambda x: \"Fiber optic\" in x)\n",
    "rdd_fiber.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['5575-GNVDE,Male,0,No,No,34,Yes,No,DSL,Yes,No,Yes,No,No,No,One year,No,Mailed check,56.95,1889.5,No',\n",
       " '3668-QPYBK,Male,0,No,No,2,Yes,No,DSL,Yes,Yes,No,No,No,No,Month-to-month,Yes,Mailed check,53.85,108.15,Yes']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sort the rdd by the Total Changes columns - desccending:\n",
    "\n",
    "rdd_sorted = rdd.sortBy(lambda line: line[19],ascending = False)\n",
    "rdd_sorted.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['customerID',\n",
       "   'gender',\n",
       "   'SeniorCitizen',\n",
       "   'Partner',\n",
       "   'Dependents',\n",
       "   'tenure',\n",
       "   'PhoneService',\n",
       "   'MultipleLines',\n",
       "   'InternetService',\n",
       "   'OnlineSecurity',\n",
       "   'OnlineBackup',\n",
       "   'DeviceProtection',\n",
       "   'TechSupport',\n",
       "   'StreamingTV',\n",
       "   'StreamingMovies',\n",
       "   'Contract',\n",
       "   'PaperlessBilling'],\n",
       "  ['MonthlyCharges', 'TotalCharges']),\n",
       " (['7590-VHVEG',\n",
       "   'Female',\n",
       "   '0',\n",
       "   'Yes',\n",
       "   'No',\n",
       "   '1',\n",
       "   'No',\n",
       "   'No phone service',\n",
       "   'DSL',\n",
       "   'No',\n",
       "   'Yes',\n",
       "   'No',\n",
       "   'No',\n",
       "   'No',\n",
       "   'No',\n",
       "   'Month-to-month',\n",
       "   'Yes'],\n",
       "  ['29.85', '29.85'])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Key Value pairs: RDD: to perform aggregations, and transform the data.\n",
    "\n",
    "\n",
    "rdd_2 = rdd_splited.map(lambda line: (line[0:17],line[18:20]))\n",
    "rdd_2.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> RDD x DataFrame x DataSets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDD: Primary user-facing API in Spark, since the beggining. About 2011.\n",
    "\n",
    "DataFrame: Distribute collection of Row objects, UDFs, logical plan optimizer. Spark 2.0. 2015.\n",
    "\n",
    "Dataset: Starting in Spark 2.0. Strongly-typed API & performed. 2016 (Scala & Java, only unitl now***).\n",
    "\n",
    "Source: https://spark.apache.org/docs/latest/sql-programming-guide.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> PySpark DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are many ways to create DataFrames in Pyspark. Lets see how it works:\n",
    "\n",
    "#a) create dataframe based on rdd. Just for an example:\n",
    "\n",
    "df = rdd_splited.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_1=1, _2=2, _3=3), Row(_1=4, _2=5, _3=6)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#b) create dataframe based on a collection:\n",
    "\n",
    "c = [(1,2,3),(4,5,6),(7,8,9),(10,11,12),(13,14,15)]\n",
    "df = spark.createDataFrame(c)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(customerID='7590-VHVEG', gender='Female', SeniorCitizen='0', Partner='Yes', Dependents='No', tenure='1', PhoneService='No', MultipleLines='No phone service', InternetService='DSL', OnlineSecurity='No', OnlineBackup='Yes', DeviceProtection='No', TechSupport='No', StreamingTV='No', StreamingMovies='No', Contract='Month-to-month', PaperlessBilling='Yes', PaymentMethod='Electronic check', MonthlyCharges='29.85', TotalCharges='29.85', Churn='No'),\n",
       " Row(customerID='5575-GNVDE', gender='Male', SeniorCitizen='0', Partner='No', Dependents='No', tenure='34', PhoneService='Yes', MultipleLines='No', InternetService='DSL', OnlineSecurity='Yes', OnlineBackup='No', DeviceProtection='Yes', TechSupport='No', StreamingTV='No', StreamingMovies='No', Contract='One year', PaperlessBilling='No', PaymentMethod='Mailed check', MonthlyCharges='56.95', TotalCharges='1889.5', Churn='No')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#c) create dataframe based on csv file (external):\n",
    "\n",
    "df = spark.read.csv(file, header=True, sep=',', inferSchema=False)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customerID: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- SeniorCitizen: string (nullable = true)\n",
      " |-- Partner: string (nullable = true)\n",
      " |-- Dependents: string (nullable = true)\n",
      " |-- tenure: string (nullable = true)\n",
      " |-- PhoneService: string (nullable = true)\n",
      " |-- MultipleLines: string (nullable = true)\n",
      " |-- InternetService: string (nullable = true)\n",
      " |-- OnlineSecurity: string (nullable = true)\n",
      " |-- OnlineBackup: string (nullable = true)\n",
      " |-- DeviceProtection: string (nullable = true)\n",
      " |-- TechSupport: string (nullable = true)\n",
      " |-- StreamingTV: string (nullable = true)\n",
      " |-- StreamingMovies: string (nullable = true)\n",
      " |-- Contract: string (nullable = true)\n",
      " |-- PaperlessBilling: string (nullable = true)\n",
      " |-- PaymentMethod: string (nullable = true)\n",
      " |-- MonthlyCharges: string (nullable = true)\n",
      " |-- TotalCharges: string (nullable = true)\n",
      " |-- Churn: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(CustomerID='7590-VHVEG'), Row(CustomerID='5575-GNVDE')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select some column\n",
    "\n",
    "df.select(\"CustomerID\").take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(gender='Female'), Row(gender='Male')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Distinct Values by column:\n",
    "\n",
    "df.select(\"gender\").distinct().take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------+\n",
      "|gender|sum(TotalCharges)|\n",
      "+------+-----------------+\n",
      "|Female|7952354.199999998|\n",
      "|  Male|8103814.499999997|\n",
      "+------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Aggregate the dataframe & create measure:\n",
    "\n",
    "df.groupby(\"gender\").agg({\"TotalCharges\":\"sum\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(customerID='9093-FPDLG', gender='Female', SeniorCitizen='0', Partner='No', Dependents='No', tenure='11', PhoneService='Yes', MultipleLines='No', InternetService='Fiber optic', OnlineSecurity='No', OnlineBackup='Yes', DeviceProtection='Yes', TechSupport='Yes', StreamingTV='No', StreamingMovies='Yes', Contract='Month-to-month', PaperlessBilling='Yes', PaymentMethod='Electronic check', MonthlyCharges='94.2', TotalCharges='999.9', Churn='No'),\n",
       " Row(customerID='4536-PLEQY', gender='Male', SeniorCitizen='0', Partner='Yes', Dependents='No', tenure='12', PhoneService='Yes', MultipleLines='No', InternetService='Fiber optic', OnlineSecurity='No', OnlineBackup='Yes', DeviceProtection='No', TechSupport='No', StreamingTV='No', StreamingMovies='Yes', Contract='Month-to-month', PaperlessBilling='Yes', PaymentMethod='Credit card (automatic)', MonthlyCharges='85.05', TotalCharges='999.8', Churn='No'),\n",
       " Row(customerID='5899-MQZZL', gender='Female', SeniorCitizen='0', Partner='No', Dependents='No', tenure='13', PhoneService='Yes', MultipleLines='Yes', InternetService='Fiber optic', OnlineSecurity='No', OnlineBackup='No', DeviceProtection='No', TechSupport='No', StreamingTV='No', StreamingMovies='No', Contract='Month-to-month', PaperlessBilling='Yes', PaymentMethod='Mailed check', MonthlyCharges='75', TotalCharges='999.45', Churn='Yes')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Order the dataframe:\n",
    "\n",
    "df.orderBy(df.TotalCharges.desc()).take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#more functions & dataframe: https://spark.apache.org/docs/latest/api/python/pyspark.sql.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
